{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining hyperparameters and certain settings\n",
    "test_run = False  # Set to False during actual training\n",
    "test_run_size = 1024  # Number of image pairs used in a test run\n",
    "training_experiment = False\n",
    "val_percent = 0.2  # Percent of images used for validation\n",
    "batch_size = 16  # batch size\n",
    "lr = 0.05  # Learning Rate\n",
    "random_seed = 99  # Don't Change. Random Seed for train_test_split.\n",
    "momentum = 0.9  # If using SGD.\n",
    "epochs = 20\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = \"dataset_stats.json\"\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "with open(json_name) as file:\n",
    "    stats = json.load(file)\n",
    "    std = [round(i, 3) for i in stats[\"std\"]]\n",
    "    mean = [round(i, 3) for i in stats[\"mean\"]]\n",
    "    n = stats[\"n\"]\n",
    "\n",
    "with open(\"train_label.txt\", \"r\") as f:\n",
    "    labels = [float(label.strip(\"\\n\")) for label in f.readlines()]\n",
    "\n",
    "def prepare_image(image):\n",
    "    global mean\n",
    "    global std\n",
    "    # Converts image to RGB\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert(\"RGB\")\n",
    "    # Adds necessary transforms to image\n",
    "    Transform = transforms.Compose([\n",
    "        # Scales the short side to 256px. Aspect ratio unchanged. Then center\n",
    "        # crops to make the size of all images equal\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop([256, 256]),\n",
    "        # Converts PIL Image to tensor\n",
    "        transforms.ToTensor(),\n",
    "        # Normalises each channel by subtracting the mean from each channel and\n",
    "        # dividing by the std\n",
    "        transforms.Normalize(mean, std)\n",
    "    ])\n",
    "    image = Transform(image)\n",
    "    return image\n",
    "\n",
    "# Given a batch of images, it applies prepare_image() on each and returns\n",
    "# a tensor of image pairs\n",
    "def prepare_images(image_numbers):\n",
    "    #unpop_images = [Image.open(\"image_dataset/\"+str(i[1][0])+\".jpg\") for i in images.iterrows()]\n",
    "    image_tensor = torch.stack([prepare_image(Image.open(\"train_images/\" + str(number) + \".jpg\")) for number in image_numbers])\n",
    "    return image_tensor\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomResizedCrop([224, 224]), transforms.RandomHorizontalFlip()])\n",
    "centre_crop = transforms.CenterCrop([224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = list(range(len(labels)))\n",
    "\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size = val_percent, random_state=random_seed, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = prepare_images(val_images[:100])\n",
    "images[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = train_transform(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IIPAModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        if model_name == \"resnet\":\n",
    "            self.model = torchvision.models.resnet50()\n",
    "            self.model.fc = torch.nn.Linear(in_features=2048, out_features=1)\n",
    "            nn.init.kaiming_uniform_(self.model.fc.weight)\n",
    "        else:\n",
    "            self.model = EfficientNet.from_pretrained(model_name, num_classes = 1)\n",
    "            nn.init.kaiming_uniform_(self.model._fc.weight)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        preds = self.model(batch)\n",
    "        return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_transform(images)[0]\n",
    "image = image.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IIPAModel(\"resnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(images)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.Tensor(val_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29.6715, grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([13.0924,  3.1055,  2.2793,  2.8653,  3.6857,  3.6473,  2.5182,  2.3701,\n",
       "         2.1547,  7.1868,  5.2493,  2.4435,  3.0774,  3.4904,  2.2854,  4.7625,\n",
       "         4.2139,  2.7889,  2.1392,  2.2298,  2.9881,  2.5720,  2.2757,  2.4911,\n",
       "         2.3010,  3.9357,  3.9951,  2.0147,  2.5054,  5.5637,  2.6945,  3.6643,\n",
       "         2.4986,  2.2204,  3.1293,  2.4174,  2.5322,  2.4223,  2.3256,  2.8337,\n",
       "         2.8229,  2.4854,  2.3356,  3.7550,  3.7580,  2.7149,  2.6535,  3.6883,\n",
       "         2.2592,  2.8344,  2.7861,  2.8379,  3.0066,  2.1901,  2.2175,  3.6012,\n",
       "         2.0512,  2.5451,  2.9383,  3.3911,  2.7839,  2.4553,  3.9779,  2.5236,\n",
       "         3.1264,  2.9413,  6.0884,  2.2943,  2.5718,  3.1930,  2.1172,  5.1770,\n",
       "         7.1296,  1.8702,  3.6414, 11.0113,  2.6712,  4.7296,  5.4220,  2.6092,\n",
       "         2.7502,  3.3478,  2.4422,  2.5901,  2.5474,  3.5971,  2.7060,  2.4622,\n",
       "         2.3436,  2.5489,  2.0326,  2.0908,  2.9739,  2.7572,  3.1074,  1.8305,\n",
       "         3.0023,  2.3739,  2.1038,  2.3091], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
